---
title: "Midterm Exam"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
Universities <- read.csv("Universities.csv")
library(tidyverse)
library(factoextra)
library(caret)
library(ISLR)

```



```{r}
#Question a)
#remove all records with missing measurements from the dataset
exam <- na.omit(Universities)
head(exam)
view(exam)
```


```{r}
#Question b)
#Data Normalization 
set.seed(100)
Uni <- exam[, c(-1, -2, -3)]
Uni <- scale(Uni)
Uni <-as.data.frame(Uni)
class(Uni)
view(Uni)
```


```{r}
# Scaling the data frame (z-score) 
distance <- get_dist(Uni)
fviz_dist(distance)
```


```{r}
#Elbow Method 
fviz_nbclust(Uni, kmeans, method = "wss")
```


The chart shows that the elbow point 3 provides the best value for k. While WSS will continue to drop for larger values of k, we have to make the tradeoff between overfitting, i.e., a model fitting both noise and signal, to a model having bias. Here, the elbow point provides that compromise where WSS, while still decreasing beyond k = 3, decreases at a much smaller rate. In other words, adding more clusters beyond 3 brings less improvement to cluster homogeneity.

```{r}
# Silhouette Method
fviz_nbclust(Uni, kmeans, method = "silhouette")

```


Three clusters seem to be reasonable for describing this data. The optimal K=3. Silhouette Method graph shows the optimal number of clusters at point 3.
```{r}
#k means algorithm 
k3 <- kmeans(Uni, centers = 3, nstart = 25) 
k3
# Visualize the output

k3$centers 
k3$size 

fviz_cluster(k3, data = Uni) # Visualize the output
```



```{r}

#Question c)
#Compare the summary statistics for each cluster and describe the cluster
#Answer : By reading the summary statistics the +ve values fall above the mean (above average) and -ve values fall below the mean (below average). For every clusters there is a pattern. The excel file is attached which shows the pattern for all the mean values. The description of the each cluster for every parameter has been expained below:

#cluster 1 : *high % of new student from top 10%, *high % of new student from top 25%, * high in-state tuition, *high out-of-state tuition, *high number of rooms, *high number of boards, *high % of faculty w/PHD, *high graduation rate

#Cluster 2 : *low applications received, low, *low acceptance rate, * low new student enrollment, *low % of new student from top 10%, *low % of new student from top 25%, *low FT undegrad, *low PT undergrad, *low in-state-tutition, *low out-state-tuition, *low number of rooms, *low number of boards, *low additional fees, *low estimated book costs, *low % of faculty w/PHD, *low graduation rate
 
#Cluster 3 : *high applications received, low, *high acceptance rate, * high new student, *high FT undegrad, *high PT undergrad, *high additional fees, *high estimated book costs, *high estimated personal $, *high student faculty ratio


```


```{r}
#Question d)
#Relationship between clusters and categorical information
catg <-exam[, c(2,3)]
exam1<- cbind(catg, k3$cluster)
view(exam1)

library(ggplot2)
ggplot(data=exam1, aes(x=exam1$State, y=exam1$`k3$cluster`, color=exam1$Public..1...Private..2.)) + geom_point()


```


#Yes there is a relation between three clusters and categorical information. The above graph shows that cluster 1 contains majority of public schools.  Clsuter 2 contains combination of combination of private and public schools. And cluster 3 contains majority of Public schools. 
```{r}
#Question e)
#What other external information can explain the contents of some or all of these clusters?
# Some external information that could help explain the data would be state of schools. We can identify cluster types for each state of schools.  

```



```{r}
#Question f)
#Consider Tufts University, which is missing some information. Compute the Euclidean distance of this record from each of the clusters that you found above

newuni <- filter(Universities, College.Name== "Tufts University")
view(newuni)

newuni <-newuni[, -c(1,2,3,10)]


#Euclidean Distance for clusters 1
dist(rbind((newuni), k3$centers[1,]))

#Euclidean Distance for clusters 2
dist(rbind((newuni), k3$centers[2,]))

#Euclidean Distance for clusters 3
dist(rbind((newuni), k3$centers[3,]))


#The Euclidean distance for cluster 3 is the smaller compared to cluster 1 and cluster 2. Hence cluster 3 is the closest to Tufts. 

#Impute the missing values for Tufts University by taking the average of the cluster
#combining the data with the cluster values
exam<- cbind(exam, k3$cluster)
view(exam)

cluster3 <- filter(exam,  k3$cluster == 3)
view(cluster3)

cluster_avg <- mean(cluster3[,c(10)])
newuni[, c(10)] <- cluster_avg
newuni[, c(10)]


#The avereage of the cluster is 3500.565
```




